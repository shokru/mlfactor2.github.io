{"version":"1","records":[{"hierarchy":{"lvl1":"Bibliography"},"type":"lvl1","url":"/bibliography","position":0},{"hierarchy":{"lvl1":"Bibliography"},"content":"","type":"content","url":"/bibliography","position":1},{"hierarchy":{"lvl1":"Preface"},"type":"lvl1","url":"/","position":0},{"hierarchy":{"lvl1":"Preface"},"content":"This book is intended to cover some advanced modelling techniques applied to equity investment strategies that are built on firm characteristics. The content is threefold. First, we try to simply explain the ideas behind most mainstream machine learning algorithms that are used in equity asset allocation. Second, we mention a wide range of academic references for the readers who wish to push a little further. Finally, we provide hands-on Python code samples that show how to apply the concepts and tools on a realistic dataset which we share to encourage reproducibility.\n\nThis book is intended to cover some advanced modelling techniques applied to equity investment strategies that are built on firm characteristics. The content is threefold. First, we try to simply explain the ideas behind most mainstream machine learning algorithms that are used in equity asset allocation. Second, we mention a wide range of academic references for the readers who wish to push a little further. Finally, we provide hands-on Python code samples that show how to apply the concepts and tools on a realistic dataset which we share to encourage reproducibility.","type":"content","url":"/","position":1},{"hierarchy":{"lvl1":"Preface","lvl2":"What this book is not about"},"type":"lvl2","url":"/#what-this-book-is-not-about","position":2},{"hierarchy":{"lvl1":"Preface","lvl2":"What this book is not about"},"content":"This book deals with machine learning (ML) tools and their applications in factor investing. Factor investing is a subfield of a large discipline that encompasses asset allocation, quantitative trading and wealth management. Its premise is that differences in the returns of firms can be explained by the characteristics of these firms. Thus, it departs from traditional analyses which rely on price and volume data only, like classical portfolio theory à la \n\nMarkowitz, 1952, or high frequency trading. For a general and broad treatment of Machine Learning in Finance, we refer to \n\nDixon et al., 2020.\n\nThe topics we discuss are related to other themes that will not be covered in the monograph. These themes include:\n\nApplications of ML in other financial fields, such as fraud detection or credit scoring. We refer to \n\nNgai et al., 2011 and \n\nBaesens et al., 2015 for general purpose fraud detection,\nto \n\nBhattacharyya et al., 2011 for a focus on credit cards and to \n\nRavisankar et al., 2011\nand \n\nAbbasi et al., 2012 for studies on fraudulent financial reporting. On the topic of credit scoring,\n\n\nWang et al., 2011 and \n\nBrown & Mues, 2012 provide overviews of methods and some empirical results.\nAlso, we do not cover ML algorithms for data sampled at higher (daily or intraday) frequencies (microstructure models, limit order book).\nThe chapter from \n\nKearns & Nevmyvaka, 2013 and the recent paper by \n\nSirignano & Cont, 2019 are good introductions on this topic.\n\nUse cases of alternative datasets that show how to leverage textual data from social media, satellite imagery, or credit card logs to predict sales, earning reports, and, ultimately, future returns.\nThe literature on this topic is still emerging (see, e.g., \n\nBlank et al., 2019,\n\n\nJha, 2019 and \n\nKe et al., 2019) but will likely blossom in the near future.\n\nTechnical details of machine learning tools. While we do provide some insights on specificities of some approaches\n(those we believe are important), the purpose of the book is not to serve as reference manual on statistical learning.\nWe refer to \n\nHastie et al., 2009, \n\nCornuejols et al., 2018 (written in French),\n\n\nJames et al., 2013 (coded in R!) and \n\nMohri et al., 2018 for a general treatment on the subject.\nMoreover, \n\nDu & Swamy, 2013 and \n\nGoodfellow et al., 2016 are solid monographs on neural networks particularly\nand \n\nSutton & Barto, 2018 provide a self-contained and comprehensive tour in reinforcement learning.\nFinally, the book does not cover methods of natural language processing (NLP) that can be used to evaluate sentiment\nwhich can in turn be translated into investment decisions. This topic has nonetheless been trending lately\nand we refer to \n\nLoughran & McDonald, 2016, \n\nCong et al., 2019, \n\nCong et al., 2019 and \n\nGentzkow et al., 2019 for recent advances on the matter.","type":"content","url":"/#what-this-book-is-not-about","position":3},{"hierarchy":{"lvl1":"Preface","lvl2":"The targeted audience"},"type":"lvl2","url":"/#the-targeted-audience","position":4},{"hierarchy":{"lvl1":"Preface","lvl2":"The targeted audience"},"content":"Who should read this book? This book is intended for two types of audiences. First, postgraduate students who wish to pursue their studies in quantitative finance with a view towards investment and asset management. The second target groups are professionals from the money management industry who either seek to pivot towards allocation methods that are based on machine learning or are simply interested in these new tools and want to upgrade their set of competences. To a lesser extent, the book can serve scholars or researchers who need a manual with a broad spectrum of references both on recent asset pricing issues and on machine learning algorithms applied to money management. While the book covers mostly common methods, it also shows how to implement more exotic models, like causal graphs (Chapter 14), Bayesian additive trees (Chapter 9), and hybrid autoencoders (Chapter 7).\n\nThe book assumes basic knowledge in algebra (matrix manipulation), analysis (function differentiation, gradients), optimization (first and second order conditions, dual forms), and statistics (distributions, moments, tests, simple estimation method like maximum likelihood). A minimal financial culture is also required: simple notions like stocks, accounting quantities (e.g., book value) will not be defined in this book. Lastly, all examples and illustrations are coded in Python. A minimal culture of the language is sufficient to understand the code snippets which rely heavily on the most common functions and libraries.","type":"content","url":"/#the-targeted-audience","position":5},{"hierarchy":{"lvl1":"Preface","lvl2":"How this book is structured"},"type":"lvl2","url":"/#how-this-book-is-structured","position":6},{"hierarchy":{"lvl1":"Preface","lvl2":"How this book is structured"},"content":"The book is divided into four parts.\n\nPart I gathers preparatory material and starts with notations and data presentation (Chapter 1), followed by introductory remarks (Chapter 2). Chapter 3 outlines the economic foundations (theoretical and empirical) of factor investing and briefly sums up the dedicated recent literature. Chapter 4 deals with data preparation. It rapidly recalls the basic tips and warns about some major issues.\n\nPart II of the book is dedicated to predictive algorithms in supervised learning. Those are the most common tools that are used to forecast financial quantities (returns, volatilities, Sharpe ratios, etc.). They range from penalized regressions (Chapter 5), to tree methods (Chapter 6), encompassing neural networks (Chapter 7), support vector machines (Chapter 8) and Bayesian approaches (Chapter 9).\n\nThe next portion of the book bridges the gap between these tools and their applications in finance. Chapter 10 details how to assess and improve the ML engines defined beforehand. Chapter 11 explains how models can be combined and often why that may not be a good idea. Finally, one of the most important chapters (Chapter 12) reviews the critical steps of portfolio backtesting and mentions the frequent mistakes that are often encountered at this stage.\n\nThe end of the book covers a range of advanced topics connected to machine learning more specifically. The first one is interpretability. ML models are often considered to be black boxes and this raises trust issues: how and why should one trust ML-based predictions? Chapter 13 is intended to present methods that help understand what is happening under the hood. Chapter 14 is focused on causality, which is both a much more powerful concept than correlation and also at the heart of many recent discussions in Artificial Intelligence (AI). Most ML tools rely on correlation-like patterns and it is important to underline the benefits of techniques related to causality. Finally, Chapters 15 and 16 are dedicated to non-supervised methods. The latter can be useful, but their financial applications should be wisely and cautiously motivated.","type":"content","url":"/#how-this-book-is-structured","position":7},{"hierarchy":{"lvl1":"Preface","lvl2":"Coding instructions"},"type":"lvl2","url":"/#coding-instructions","position":8},{"hierarchy":{"lvl1":"Preface","lvl2":"Coding instructions"},"content":"One of the purposes of the book is to propose a large-scale tutorial of ML applications in financial predictions and portfolio selection. Thus, one keyword is REPRODUCIBILITY! In order to duplicate our results (up to possible randomness in some learning algorithms), you will need running versions of Python and Anaconda on your computer.\n\ntexte du lienA list of the packages we use can be found in the Table below.\n\nPackage\n\nPurpose\n\nChapter(s)\n\nshort\n\npandas\n\nMultiple usage\n\nalmost all\n\npd\n\nurllib.request\n\nData from url\n\n3\n\n\n\nstatsmodels\n\nStatistical regression\n\n3, 4, 14,15, 16\n\nsm\n\nnumpy\n\nMultiple usage\n\nalmost all\n\nnp\n\nmatplotlib\n\nPlotting\n\nalmost all\n\nplt\n\nseaborn\n\nPlotting\n\n4,6,15\n\nsns\n\nIPython.display\n\nTable display\n\n4\n\n\n\nsklearn\n\nMachine learning\n\n5,6,7,8,9,10,11,15\n\n\n\nxgboost\n\nMachine learning\n\n6,10,12\n\nxgb\n\ntensorflow\n\nMachine learning\n\n7,11\n\ntf\n\nplot_keras_history\n\nPlotting\n\n7,11\n\n\n\nxbart\n\nBayesian trees\n\n9\n\n\n\nskopt\n\nBayesian optimisation\n\n10\n\n\n\ncvxopt\n\nOptimisation\n\n11\n\ncvx\n\ndatetime\n\ndate functions\n\n12\n\n\n\nitertools\n\nIterate utils\n\n12\n\n\n\nscipy\n\nOptimisation\n\n12\n\n\n\nrandom\n\nStatistics\n\n13\n\n\n\ncollections\n\nUtils\n\n13\n\n\n\nlime\n\nInterpretability\n\n13\n\n\n\nshap\n\nInterpretability\n\n13\n\n\n\ndalex\n\nInterpretability\n\n13\n\n\n\ncausalimpact\n\nCausality\n\n14\n\n\n\ncdt\n\nCausality\n\n14\n\n\n\nnetworks\n\nGraph / Causality\n\n14\n\nnx\n\nicpy\n\nCausality\n\n14\n\nicpy\n\npca\n\nPlotting pca\n\n15\n\n\n\nAs much as we could, we created short code chunks and commented each line whenever we felt it was useful. Comments are displayed at the end of a row and preceded with a single hastag #.\n\nThe book is constructed as a very big notebook, thus results are often presented below code chunks. They can be graphs or tables. Sometimes, they are simple numbers and are preceded with two hashtags ##. The example below illustrates this formatting.\n\nThe book can be viewed as a very big tutorial. Therefore, most of the chunks depend on previously defined variables. When replicating parts of the code (via online code), please make sure that the environment includes all relevant variables. One best practice is to always start by running all code chunks from Chapter 1. For the exercises, we often resort to variables created in the corresponding chapters.","type":"content","url":"/#coding-instructions","position":9},{"hierarchy":{"lvl1":"Preface","lvl2":"Acknowledgments"},"type":"lvl2","url":"/#acknowledgments","position":10},{"hierarchy":{"lvl1":"Preface","lvl2":"Acknowledgments"},"content":"The core of the book was originally prepared for a series of lectures given by one of the authors to students of master’s degrees in finance at EMLYON Business School and at the Imperial College Business School in the Spring of 2019. We are grateful to those students who asked fruitful questions and thereby contributed to improve the content of the book.\n\nFor the first edition, we were grateful to Bertrand Tavin and Gautier Marti for their thorough screening of the book. We also thanked Eric André, Aurélie Brossard, Alban Cousin, Frédérique Girod, Philippe Huber, Jean-Michel Maeso, Javier Nogales and for friendly reviews; Christophe Dervieux for his help with bookdown; Mislav Sagovac and Vu Tran for their early feedback; Lara Spieker, John Kimmel for making it happen and Jonathan Regenstein for his availability, no matter the topic. Lastly, we were grateful for the anonymous reviews collected by John, our original editor.\n\nThe second version has benefitted from a large number of conversations with academics and practitioners, including Arnaud Battistella, Jean-Charles Bertrand, Guillaume Chevallier, Jean-Michel Maeso (again!), Nicholas McLoughlin, Thomas Raffinot.\n\nFinally, we must acknowledge great help by several LLMs for the processing of references while recycling old files, especially Claude. Life is not the same without them, yet we must use them with caution.","type":"content","url":"/#acknowledgments","position":11},{"hierarchy":{"lvl1":"Notations and data"},"type":"lvl1","url":"/chap-01-notations","position":0},{"hierarchy":{"lvl1":"Notations and data"},"content":"","type":"content","url":"/chap-01-notations","position":1},{"hierarchy":{"lvl1":"Notations and data","lvl2":"Notations"},"type":"lvl2","url":"/chap-01-notations#notations","position":2},{"hierarchy":{"lvl1":"Notations and data","lvl2":"Notations"},"content":"This section aims at providing the formal mathematical conventions that will be used throughout the book.\n\nBold notations indicate vectors and matrices. We use capital letters for matrices and lower case letters for vectors. \\mathbf{v}' and \\mathbf{M}' denote the transposes of \\mathbf{v} and \\mathbf{M}. \\mathbf{M}=[m]_{i,j} where i is the row index and j the column index.\n\nWe will work with two notations in parallel. The first one is the pure machine learning notation in which the labels (also called output, dependent variables or predicted variables) \\mathbf{y}=y_i are approximated by functions of features \\mathbf{X}_i=(x_{i,1},\\dots,x_{i,K}). The dimension of the features matrix \\mathbf{X} is I\\times K: there are I instances, records, or observations and each one of them has K attributes, features, inputs, or predictors which will serve as independent and explanatory variables (all these terms will be used interchangeably). Sometimes, to ease notations, we will write \\textbf{x}_i for one instance (one row) of \\textbf{X} or \\textbf{x}_k for one (feature) column vector of \\textbf{X}.\n\nThe second notation type pertains to finance and will directly relate to the first. We will often work with discrete returns r_{t,n}=p_{t,n}/p_{t-1,n}-1 computed from price data. Here t is the time index and n the asset index. Unless specified otherwise, the return is always computed over one period, though this period can sometimes be one month or one year. Whenever confusion might occur, we will specify other notations for returns.\n\nIn line with our previous conventions, the number of return dates will be T and the number of assets, N. The features or characteristics of assets will be denoted with x_{t,n}^{(k)}: it is the time-t value of the k^{th} attribute of firm or asset n. In stacked notation, \\mathbf{x}_{t,n} will stand for the vector of characteristics of asset n at time t. Moreover, \\mathbf{r}_t stands for all returns at time t while \\mathbf{r}_n stands for all returns of asset n. Often, returns will play the role of the dependent variable, or label (in ML terms). For the riskless asset, we will use the notation r_{t,f}.\n\nThe link between the two notations will most of the time be the following. One instance (or observation) i will consist of one couple (t,n) of one particular date and one particular firm (if the data is perfectly rectangular with no missing field,I=T\\times N). The label will usually be some performance measure of the firm computed over some future period, while the features will consist of the firm attributes at time-t. Hence, the purpose of the machine learning engine in factor investing will be to determine the model that maps the time-t characteristics of firms to their future performance.\n\nIn terms of canonical matrices: \\mathbf{I}_N will denote the (N\\times N) identity matrix.\n\nFrom the probabilistic literature, we employ the expectation operator \\mathbb{E}[\\cdot] and the conditional expectation \\mathbb{E}_t[\\cdot], where the corresponding filtration \\mathcal{F}_t corresponds to all information available at time . More precisely, \\mathbb{E}_t[\\cdot]=\\mathbb{E}[\\cdot | \\mathcal{F}_t] \\mathbb{V}[\\cdot] will denote the variance operator. Depending on the context, probabilities will be written simply P, but sometimes we will use the heavier notation \\mathbb{P}. Probability density functions (pdfs) will be denoted with lowercase letters (f) and cumulative distribution functions (cdfs) with uppercase letters (F). We will write equality in distribution as X \\overset{d}{=}Y, which is equivalent to F_X(z)=F_Y(z) for all z on the support of the variables. For a random process X_t, we say that it is stationary if the law of X_t is constant through time, i.e., X_t\\overset{d}{=}X_s, where \\overset{d}{=} means equality in distribution.\n\nSometimes, asymptotic behaviors will be characterized with the usual Landau notation o(\\cdot) and O(\\cdot). The symbol \\propto refers to proportionality: x\\propto y means that x is proportional to y. With respect to derivatives, we use the standard notation \\frac{\\partial}{\\partial x} when differentiating with respect to x. We resort to the compact symbol \\nabla when all derivatives are computed (gradient vector).\n\nIn equations, the left-hand side and right-hand side can be written more compactly: l.h.s. and r.h.s., respectively.\n\nFinally, we turn to functions. We list a few below:\n\n1_{\\{x \\}}: the indicator function of the condition x, which is equal to one if x\nis true and to zero otherwise.\n\n\\phi(\\cdot) and \\Phi(\\cdot) are the standard Gaussian pdf and cdf.\n\ncard (\\cdot)=\\#(\\cdot) are two notations for the cardinal function which evaluates the number of elements in a given set (provided as argument of the function).\n\n\\lfloor \\cdot \\rfloor is the integer part function.\n\nfor a real number x,[x]^+\nis the positive part of x, that is max \\max(0,x)\n\ntanh(\\cdot) is the hyperbolic tangent: tanh(x)=\\frac{e^x-e^{-x}}{e^x+e^{-x}}\n\nReLu(\\cdot) is the rectified linear unit: ReLu(x)=\\max(0,x)\n\ns(\\cdot) will be the softmax function: s(\\textbf{x})_i=\\frac{e^{x_i}}{\\sum_{j=1}^Je^{x_j}}, where the subscript i refers to the i^{th} element of the vector.\n\n","type":"content","url":"/chap-01-notations#notations","position":3},{"hierarchy":{"lvl1":"Notations and data","lvl2":"Dataset"},"type":"lvl2","url":"/chap-01-notations#dataset","position":4},{"hierarchy":{"lvl1":"Notations and data","lvl2":"Dataset"},"content":"","type":"content","url":"/chap-01-notations#dataset","position":5},{"hierarchy":{"lvl1":"Notations and data","lvl3":"Presentation","lvl2":"Dataset"},"type":"lvl3","url":"/chap-01-notations#presentation","position":6},{"hierarchy":{"lvl1":"Notations and data","lvl3":"Presentation","lvl2":"Dataset"},"content":"Throughout the book, and for the sake of reproducibility, we will illustrate the concepts we present with examples of implementation based on a single financial dataset available at \n\nhttps://​github​.com​/shokru​/mlfactor​.github​.io​/tree​/master​/material. This dataset comprises information on 1,207 stocks listed in the US (possibly originating from Canada or Mexico). The time range starts in November XXX and ends in March XXXX. For each point in time, 123 characteristics describe the firms in the sample. These attributes cover a wide range of topics:\n\nvaluation (earning yields, accounting ratios);\n\nprofitability and quality (return on equity);\n\nmomentum and technical analysis (past returns, relative strength index);\n\nrisk (volatilities);\n\nestimates (earnings-per-share);\n\nvolume and liquidity (share turnover).\n\nThe sample is not perfectly rectangular (balanced): there are no missing points, but the number of firms and their attributes is not constant through time. This makes the computations in the backtest more tricky, but also more realistic.\n\n","type":"content","url":"/chap-01-notations#presentation","position":7},{"hierarchy":{"lvl1":"Notations and data","lvl3":"Exploration","lvl2":"Dataset"},"type":"lvl3","url":"/chap-01-notations#exploration","position":8},{"hierarchy":{"lvl1":"Notations and data","lvl3":"Exploration","lvl2":"Dataset"},"content":"\n\nimport pandas as pd                                  # Activate the data science package\ndata_raw=pd.read_csv('factors_map_all_US.csv')       # Load the data\nidx_date=data_raw.index[(data_raw['date'] > '1995-12-31') & (data_raw['date'] < '2025-06-01')].tolist() # creating and index to retrive the dates\ndata_ml=data_raw.iloc[idx_date]                      # filtering the dataset according to date index\ndata_ml = data_ml.drop(columns = 'Unnamed: 0')\n\n\n\ndata_ml.iloc[0:6,0:6]\n\n\n\ndata_ml.shape\n\n\n\ndata_ml.columns\n\n\n\nThe data has 129 columns and 159,396 rows. The first two columns indicate the stock identifier and the date. The next 123 columns are the features (see Table XXX in the Appendix for details). The last four columns are the labels. The points are sampled at the monthly frequency. As is often the case in practice, the number of assets changes with time, as is shown below.\n\nimport matplotlib.pyplot as plt\npd.Series(data_ml.groupby('date').size()).plot(figsize=(10,4)) # counting the number of assets for each date\nplt.ylabel('nb_assets')                                        # adding the ylabel and plotting \n\n\n\nThere are four immediate labels in the dataset: R1M, R3M, R6M and R12M, which correspond to the 1-month, 3-month, 6-month and 12-month future/forward returns of the stocks. The returns are total returns, that is, they incorporate potential dividend payments over the considered periods. This is a better proxy of financial gain compared to price returns only. We refer to the analysis of \n\nHartzmark & Solomon, 2019 for a study on the impact of decoupling price returns and dividends. These labels are located in the last 4 columns of the dataset. We provide their descriptive statistics below.\n\n(data_ml[[\"R1M\", \"R3M\", \"R6M\", \"R12M\"]]\n    .describe()\n    .T\n    .loc[:, [\"mean\", \"std\", \"min\", \"25%\", \"50%\", \"75%\", \"max\"]]\n    .round(3))\n\n\n\n\nIn anticipation for future models, we keep the name of the predictors in memory. In addition, we also keep a much shorter list of predictors.\n\nfeatures=list(data_ml.iloc[:,3:125].columns)\n# Keep the feature's column names (hard-coded, beware!)\nfeatures_short =[\"Div_yld\", \"EPS\", \"Size12m\",\n                 \"Mom_LT\", \"Ocf\", \"PB\", \"Vol_LT\"]\n\n\n\nThe predictors have been uniformized, that is, for any given feature and time point, the distribution is uniform. Imputation has also been performed, whereby any missing point is assigned the value 0.5. Below, we represent the distribution of dividend yield values, with all imputed values removed. Given roughly 800 stocks, the graph below cannot display a perfect rectangle.\n\ndata_ml.query('Div_yld != 0.5')['Div_yld'].hist(bins=100)\n# using the hist\nplt.ylabel('count')\n\n\n\nThe original labels (future returns) are numerical and will be used for regression exercises, that is, when the objective is to predict a scalar real number. Sometimes, the exercises can be different and the purpose may be to forecast categories (also called classes), like “buy”, “hold” or “sell”. In order to be able to perform this type of classification analysis, we create additional labels that are categorical.\n\nimport numpy as np\ndf_median=[]  #creating empty placeholder for temporary dataframe\ndf=[]         #creating empty placeholder for temporary dataframe\ndf_median=data_ml[['date','R1M','R12M']].groupby(\n    ['date']).median() # computings medians for both labels at each date\ndf_median.rename(\n    columns={\"R1M\": \"R1M_median\",\n             \"R12M\": \"R12M_median\"},inplace=True)\ndf = pd.merge(data_ml,df_median,how='left', on=['date'])\n# join the dataframes\ndata_ml['R1M_C'] = np.where( # Create the categorical labels\n    df['R1M'] > df['R1M_median'], 1.0, 0.0)\ndata_ml['R12M_C'] = np.where( # Create the categorical labels\n    df['R12M'] > df['R12M_median'], 1.0, 0.0)\n\n\n\nThe new labels are binary: they are equal to 1 (true) if the original return is above that of the median return over the considered period and to 0 (false) if not. Hence, at each point in time, half of the sample has a label equal to zero and the other half to one: some stocks overperform and others underperform.\n\nIn machine learning, models are estimated on one portion of data (training set) and then tested on another portion of the data (testing set) to assess their quality. We split our sample accordingly.\n\nseparation_date = \"2017-01-15\"\nidx_train=data_ml.index[(data_ml['date']< separation_date)].tolist()\nidx_test=data_ml.index[(data_ml['date']>= separation_date)].tolist()\n\n\n\nWe also keep in memory a few key variables, like the list of asset identifiers and a rectangular version of returns. For simplicity, in the computation of the latter, we shrink the investment universe to keep only the stocks for which we have the maximum number of points. We provide a snapshot of the corresponding matrix.\n\nstock_ids_short=[]   # empty placeholder for temporary dataframe\nstock_days=[]        # empty placeholder for temporary dataframe\nstock_ids=data_ml['fsym_id'].unique() # A list of all stock_ids\nstock_days=data_ml[['date','fsym_id']].groupby(\n    ['fsym_id']).count().reset_index() # compute nbr data points/stock\nstock_ids_short=stock_days.loc[\n    stock_days['date'] == (stock_days['date'].max())]\n# Stocks with full data\nstock_ids_short=stock_ids_short['fsym_id'].unique()\n# in order to get a list\nis_stock_ids_short=data_ml['fsym_id'].isin(stock_ids_short)\nreturns=data_ml[is_stock_ids_short].pivot(\n    index='date',columns='fsym_id',values='R1M') # returns matrix\nreturns.iloc[0:5,0:5]\n\n","type":"content","url":"/chap-01-notations#exploration","position":9},{"hierarchy":{"lvl1":"Introduction"},"type":"lvl1","url":"/chap-02-introduction","position":0},{"hierarchy":{"lvl1":"Introduction"},"content":"Conclusions often echo introductions. This chapter was completed at the very end of the writing of the book. It outlines principles and ideas that are probably more relevant than the sum of technical details covered subsequently. When stuck with disappointing results, we advise the reader to take a step away from the algorithm and come back to this section to get a broader perspective of some of the issues in predictive modelling.\n\n","type":"content","url":"/chap-02-introduction","position":1},{"hierarchy":{"lvl1":"Introduction","lvl2":"Context"},"type":"lvl2","url":"/chap-02-introduction#context","position":2},{"hierarchy":{"lvl1":"Introduction","lvl2":"Context"},"content":"\n\nThe blossoming of machine learning in factor investing has it source at the confluence of three favorable developments: data availability, computational capacity, and economic groundings.\n\nFirst, the data. Nowadays, classical providers, such as Bloomberg and Reuters have seen their playing field invaded by niche players and aggregation platforms. In addition, high-frequency data and derivative quotes have become mainstream. Hence, firm-specific attributes are easy and often cheap to compile. This means that the size of \\mathbf{X} in (2.1) is now sufficiently large to be plugged into ML algorithms. The order of magnitude (in 2025, it has not changed much from 2019 in fact) that can be reached is the following:\n\na few hundred monthly observations (a few decades)\n\nover several thousand stocks (US listed at least)\n\ncovering a few hundred attributes (sometimes a few thousand, with possible colinearity).\n\nThis makes a dataset of dozens of millions of points. While it is a reasonably high figure, we highlight that the chronological depth is probably the weak point and will remain so for decades to come because accounting figures are only released on a quarterly basis. This can somewhat be alleviated when generating synthetic data, though the efficiency of such strategy remains to be demonstrated. Needless to say that this drawback does not hold for high-frequency strategies that rely on intraday data (out of the scope of the present monograph).\n\nSecond, computational power, both through hardware and software. Storage and processing speed are not technical hurdles anymore and models can even be run on the cloud thanks to services hosted by major actors (Amazon, Microsoft, IBM and Google) and by smaller players (Rackspace, Techila). On the software side, open source has become the norm, funded by corporations (TensorFlow & Keras by Google, Pytorch by META/Facebook, h2o, etc.), universities (Scikit-Learn by INRIA, NLPCore by Stanford, NLTK by UPenn) and small groups of researchers (caret, xgboost, catboost, lightGBM, tidymodels to list but a pair of frameworks). Consequently, ML is no longer the private turf of a handful of expert computer scientists, but is on the contrary accessible to anyone willing to learn and code. Moreover, with the advent of Large Languages Models, coding has never been easier.\n\nFinally, economic framing. Machine learning applications in finance were initially introduced by computer scientists and information system experts (e.g., \n\nBraun & Chandler, 1987, \n\nWhite, 1988) and exploited shortly after by academics in financial economics (\n\nBansal & Viswanathan, 1993), and hedge funds (see, e.g., \n\nZuckerman, 2019). Nonlinear relationships then became more mainstream in asset pricing (\n\nFreeman & Tse, 1992, \n\nBansal et al., 1993). These contributions started to pave the way for the more brute-force approaches that have blossomed since the 2010 decade and which are mentioned throughout the book.\n\nIn the synthetic proposal of \n\nArnott et al., 2019, the first piece of advice is to rely on a model that makes sense economically. We agree with this stance, and the only assumption that we make in this book is that future returns depend on firm characteristics. The relationship between these features and performance is largely unknown and probably time-varying. This is why ML can be useful: to detect some hidden patterns beyond the documented asset pricing anomalies. Moreover, dynamic training allows to adapt to changing market conditions.\n\n","type":"content","url":"/chap-02-introduction#context","position":3},{"hierarchy":{"lvl1":"Introduction","lvl2":"Portfolio construction: the workflow"},"type":"lvl2","url":"/chap-02-introduction#portfolio-construction-the-workflow","position":4},{"hierarchy":{"lvl1":"Introduction","lvl2":"Portfolio construction: the workflow"},"content":"\n\nBuilding successful portfolio strategies requires many steps. This book covers many of them but focuses predominantly on the prediction part. Indeed, allocating to assets most of the time requires to make bets and thus to presage and foresee which ones will do well and which ones will not. In this book, we mostly resort to supervised learning to forecast returns in the cross-section. The baseline equation in supervised learning,\n\n\\mathbf{y}=f(\\mathbf{X})+\\mathbf{\\epsilon},\n\nis translated in financial terms as\n\n\\mathbf{r}_{t+1,n}=f(\\mathbf{x}_{t,n})+\\mathbf{\\epsilon}_{t+1,n},\n\nwhere f(\\mathbf{x}_{t,n}) can be viewed as the expected return for time t+1 computed at time t, that is, \\mathbb{E}_t[r_{t+1,n}]. Note that the model is common to all assets (f is not indexed by  n), thus it shares similarity with panel approaches.\n\nBuilding accurate predictions requires to pay attention to all terms in the above equation. Chronologically, the first step is to gather data and to process it (see Chapter 4). To the best of our knowledge, the only consensus is that, on the \\textbf{x} side, the features should include classical predictors reported in the literature: market capitalization, accounting ratios, risk measures, momentum proxies (see Chapter 3). For the dependent variable, many researchers and practitioners work with monthly returns, but other maturities may perform better out-of-sample.\n\n\nSimplified workflow in ML-based portfolio construction.\n\n","type":"content","url":"/chap-02-introduction#portfolio-construction-the-workflow","position":5},{"hierarchy":{"lvl1":"Introduction","lvl2":"Machine Learning is no magic wand"},"type":"lvl2","url":"/chap-02-introduction#machine-learning-is-no-magic-wand","position":6},{"hierarchy":{"lvl1":"Introduction","lvl2":"Machine Learning is no magic wand"},"content":"\n\nBy definition, the curse of predictions is that they rely on past data to infer patterns about subsequent fluctuations. The more or less explicit hope of any forecaster is that the past will turn out to be a good approximation of the future. Needless to say, this is a pious wish; in general, predictions fare badly. Surprisingly, this does not depend much on the sophistication of the econometric tool. In fact, heuristic guesses are often hard to beat.\n\nTo illustrate this sad truth, the baseline algorithms that we detail in Chapters 5 to 7 yield at best mediocre results. This is done on purpose. This forces the reader to understand that blindly feeding data and parameters to a coded function will seldom suffice to reach satisfactory out-of-sample accuracy.\n\nIn machine learning, models are estimated on one portion of data (training set) and then tested on another portion of the data (testing set) to assess their quality. We split our sample accordingly.\n\nBelow, we sum up some key points that we have learned through our exploratory journey in financial ML.\n\nThe first point is that causality is key. If one is able to identify X \\rightarrow y, where y are expected returns, then the problem is solved. Unfortunately, causality is incredibly hard to uncover.\n\nThus, researchers have most of the time to make do with simple correlation patterns, which are far less informative and robust.\n\nRelatedly, financial datasets are extremely noisy. It is a daunting task to extract signals out of them. No-arbitrage reasonings imply that if a simple pattern yielded durable profits, it would mechanically and rapidly vanish.\n\nThe no-free lunch theorem of \n\nWolpert, 1992 imposes that the analyst formulates views on the model. This is why economic or econometric framing is key. The assumptions and choices that are made regarding both the dependent variables and the explanatory features are decisive. As a corollary, data is key. The inputs given to the models are probably much more important than the choice of the model itself.\n\nTo maximize out-of-sample efficiency, the right question is probably to paraphrase Jeff Bezos: what’s not going to change? Persistent series are more likely to unveil enduring patterns.\n\nEverybody makes mistakes. Errors in loops or variable indexing are part of the journey. What matters is to learn from those lapses.\n\nTo conclude, we remind the reader of this obvious truth: nothing will ever replace practice. Gathering and cleaning data, coding backtests, tuning ML models, testing weighting schemes, debugging, starting all over again: these are all absolutely indispensable steps and tasks that must be repeated indefinitely. There is no sustitute to experience.","type":"content","url":"/chap-02-introduction#machine-learning-is-no-magic-wand","position":7},{"hierarchy":{"lvl1":"Factor investing and asset pricing anomalies"},"type":"lvl1","url":"/chap-03-asset-pricing","position":0},{"hierarchy":{"lvl1":"Factor investing and asset pricing anomalies"},"content":"%run libraries_data.py\n\n\n\nAsset pricing anomalies are the foundations of factor investing. In this chapter our aim is twofold:\n\npresent simple ideas and concepts: basic factor models and common empirical facts (time-varying nature of returns and risk premia);\n\nprovide the reader with lists of articles that go much deeper to stimulate and satisfy curiosity.\n\nThe purpose of this chapter is not to provide a full treatment of the many topics related to factor investing. Rather, it is intended to give a broad overview and cover the essential themes so that the reader is guided towards the relevant references. As such, it can serve as a short, non-exhaustive, review of the literature. The subject of factor modelling in finance is incredibly vast and the number of papers dedicated to it is substantial and still rapidly increasing.\n\nThe universe of peer-reviewed financial journals can be split in two. The first kind is the academic journals. Their articles are mostly written by professors, and the audience consists mostly of scholars. The articles are long and often technical. Prominent examples are the Journal of Finance, the Review of Financial Studies and the Journal of Financial Economics. The second type is more practitioner-orientated. The papers are shorter, easier to read, and target finance professionals predominantly. Two emblematic examples are the Journal of Portfolio Management and the Financial Analysts Journal. This chapter reviews and mentions articles published essentially in the first family of journals.\n\nBeyond academic articles, several monographs are already dedicated to the topic of style allocation (a synonym of factor investing used for instance in theoretical articles \n\nBarberis & Shleifer, 2003 or practitioner papers \n\nAsness et al., 2015). To cite but a few, we mention:\n\nIlmanen, 2011: an exhaustive excursion into risk premia, across many asset classes, with a large spectrum of descriptive statistics (across factors and periods),\n\nAng, 2014: covers factor investing with a strong focus on the money management industry,\n\nBali et al., 2016: very complete book on the cross-section of signals with statistical analyses (univariate metrics, correlations, persistence, etc.),\n\nJurczenko, 2017: a tour on various topics given by field experts (factor purity, predictability, selection versus weighting, factor timing, etc.).\n\nFinally, we mention a few wide-scope papers on this topic: \n\nGoyal, 2012, \n\nCazalet & Roncalli, 2014 and \n\nBaz et al., 2015.\n\n","type":"content","url":"/chap-03-asset-pricing","position":1},{"hierarchy":{"lvl1":"Factor investing and asset pricing anomalies","lvl2":"Introduction"},"type":"lvl2","url":"/chap-03-asset-pricing#introduction","position":2},{"hierarchy":{"lvl1":"Factor investing and asset pricing anomalies","lvl2":"Introduction"},"content":"\n\nThe topic of factor investing, though a decades-old academic theme, has gained traction concurrently with the rise of equity traded funds (ETFs) as vectors of investment. Both have gathered momentum in the 2010 decade. Not so surprisingly, the feedback loop between practical financial engineering and academic research has stimulated both sides in a mutually beneficial manner. Practitioners rely on key scholarly findings (e.g., asset pricing anomalies) while researchers dig deeper into pragmatic topics (e.g., factor exposure or transaction costs). Recently, researchers have also tried to quantify and qualify the impact of factor indices on financial markets. For instance, \n\nKrkoska & Schenk-Hoppé, 2019 analyze herding behaviors while \n\nCong & Xu, 2019 show that the introduction of composite securities increases volatility and cross-asset correlations.\n\nThe core aim of factor models is to understand the drivers of asset prices. Broadly speaking, the rationale behind factor investing is that the financial performance of firms depends on factors, whether they be latent and unobservable, or related to intrinsic characteristics (like accounting ratios for instance). Indeed, as \n\nCochrane, 2011 frames it, the first essential question is which characteristics really provide independent information about average returns? Answering this question helps understand the cross-section of returns and may open the door to their prediction.\n\nTheoretically, linear factor models can be viewed as special cases of the arbitrage pricing theory (APT) of \n\nRoss, 1976, which assumes that the return of an asset n can be modelled as a linear combination of underlying factors f_k:\n\n\\tag{3.1}\nr_{t,n}= \\alpha_n+\\sum_{k=1}^K\\beta_{n,k}f_{t,k}+\\epsilon_{t,n},\n\nwhere the usual econometric constraints on linear models hold: \\mathbb{E}[\\epsilon_{t,n}]=0, \\text{cov}(\\epsilon_{t,n},\\epsilon_{t,m})=0 for n\\neq m and \\text{cov}(\\textbf{f}_n,\\boldsymbol{\\epsilon}_n)=0. If such factors do exist, then they are in contradiction with the cornerstone model in asset pricing: the capital asset pricing model (CAPM) of \n\nSharpe, 1964, \n\nLintner, 1965 and \n\nMossin, 1966. Indeed, according to the CAPM, the only driver of returns is the market portfolio. This explains why factors are also called ‘anomalies’.\n\nEmpirical evidence of asset pricing anomalies has accumulated since the dual publication of \n\nFama & French, 1992 and \n\nFama & French, 1993. This seminal work has paved the way for a blossoming stream of literature that has its meta-studies (e.g., \n\nGreen et al., 2013, \n\nHarvey et al., 2016 and \n\nMcLean & Pontiff, 2016). The regression (3.1) can be evaluated once (unconditionally) or sequentially over different time frames. In the latter case, the parameters (coefficient estimates) change and the models are thus called conditional (we refer to \n\nAng & Kristensen, 2012 and to \n\nCooper & Maio, 2019 for recent results on this topic as well as for a detailed review on the related research). Conditional models are more flexible because they acknowledge that the drivers of asset prices may not be constant, which seems like a reasonable postulate.\n\n","type":"content","url":"/chap-03-asset-pricing#introduction","position":3},{"hierarchy":{"lvl1":"Factor investing and asset pricing anomalies","lvl2":"Detecting anomalies"},"type":"lvl2","url":"/chap-03-asset-pricing#detecting-anomalies","position":4},{"hierarchy":{"lvl1":"Factor investing and asset pricing anomalies","lvl2":"Detecting anomalies"},"content":"\n\n","type":"content","url":"/chap-03-asset-pricing#detecting-anomalies","position":5},{"hierarchy":{"lvl1":"Factor investing and asset pricing anomalies","lvl3":"Challenges","lvl2":"Detecting anomalies"},"type":"lvl3","url":"/chap-03-asset-pricing#challenges","position":6},{"hierarchy":{"lvl1":"Factor investing and asset pricing anomalies","lvl3":"Challenges","lvl2":"Detecting anomalies"},"content":"\n\nObviously, a crucial step is to be able to identify an anomaly and the complexity of this task should not be underestimated. Given the publication bias towards positive results (see, e.g., \n\nHarvey, 2017 in financial economics), researchers are often tempted to report partial results that are sometimes invalidated by further studies. The need for replication is therefore high and many findings have no tomorrow (\n\nLinnainmaa & Roberts, 2018, \n\nJohannesson et al., 2020), especially if transaction costs are taken into account (\n\nPatton & Weller, 2020, \n\nChen & Velikov, 2023). Nevertheless, as is demonstrated by \n\nChen, 2020, p-hacking alone cannot account for all the anomalies documented in the literature. One way to reduce the risk of spurious detection is to increase the hurdles (often, the t-statistics) but the debate is still ongoing (\n\nHarvey et al., 2016, \n\nChen, 2025), or to resort to multiple testing (\n\nHarvey et al., 2020, \n\nVincent et al., 2020).\n\nSome researchers document fading anomalies because of publication: once the anomaly becomes public, agents invest in it, which pushes prices up and the anomaly disappears. \n\nMcLean & Pontiff, 2016 document this effect in the US but \n\nJacobs & Müller, 2020 find that all other countries experience sustained post-publication factor returns. With a different methodology, \n\nChen & Zimmermann, 2020 introduce a publication bias adjustment for returns and the authors note that this (negative) adjustment is in fact rather small. \n\nPenasse, 2022 recommends the notion of alpha decay to study the persistence or attenuation of anomalies.\n\nThe destruction of factor premia may be due to herding (\n\nKrkoska & Schenk-Hoppé, 2019, \n\nVolpati et al., 2020) and could be accelerated by the democratization of so-called smart-beta products (ETFs notably) that allow investors to directly invest in particular styles (value, low volatility, etc.). For a theoretical perspective on the attractivity of factor investing, we refer to \n\nJin, 2019. On the other hand, \n\nDeMiguel et al., 2025 argue that the price impact of crowding in the smart-beta universe is mitigated by trading diversification stemming from external institutions that trade according to strategies outside this space (e.g., high frequency traders betting via order-book algorithms).\n\nThe remainder of this subsection was inspired from \n\nBaker et al., 2017 and \n\nHarvey & Liu, 2019.\n\n","type":"content","url":"/chap-03-asset-pricing#challenges","position":7},{"hierarchy":{"lvl1":"Factor investing and asset pricing anomalies","lvl3":"Simple portfolio sorts","lvl2":"Detecting anomalies"},"type":"lvl3","url":"/chap-03-asset-pricing#simple-portfolio-sorts","position":8},{"hierarchy":{"lvl1":"Factor investing and asset pricing anomalies","lvl3":"Simple portfolio sorts","lvl2":"Detecting anomalies"},"content":"\n\nThis is the most common procedure and the one used in \n\nFama & French, 1992. The idea is simple. On one date,\n\nrank firms according to a particular criterion (e.g., size, book-to-market ratio);\n\nform J\\ge 2 portfolios (i.e., homogeneous groups) consisting of the same number of stocks according to the ranking (usually, J=2, J=3, J=5 or J=10 portfolios are built, based on the median, terciles, quintiles or deciles of the criterion);\n\nthe weight of stocks inside the portfolio is either uniform (equal weights), or proportional to market capitalization;\n\nat a future date (usually one month), report the returns of the portfolios. Then, iterate the procedure until the chronological end of the sample is reached.\n\nThe outcome is a time series of portfolio returns r_t^j for each grouping j. An anomaly is identified if the t-test between the first (j=1) and the last group (j=J) unveils a significant difference in average returns. More robust tests are described in \n\nCattaneo et al., 2020. A strong limitation of this approach is that the sorting criterion could have a non-monotonic impact on returns and a test based on the two extreme portfolios would not detect it. Several articles address this concern: \n\nPatton & Timmermann, 2010 and \n\nRomano & Wolf, 2013 for instance. Another concern is that these sorted portfolios may capture not only the priced risk associated with the characteristic but also some unpriced risk. \n\nDaniel et al., 2020 show that it is possible to disentangle the two and make the most of altered sorted portfolios.\n\nInstead of focusing on only one criterion, it is possible to group assets according to more characteristics. The original paper \n\nFama & French, 1992 also combines market capitalization with book-to-market ratios. Each characteristic is divided into 10 buckets, which makes 100 portfolios in total. Beyond data availability, there is no upper bound on the number of features that can be included in the sorting process. In fact, some authors investigate more complex sorting algorithms that can manage a potentially large number of characteristics (see e.g., \n\nFeng et al., 2020 and \n\nBryzgalova et al., 2023).\n\nFinally, we refer to \n\nLedoit et al., 2020 for refinements that take into account the covariance structure of asset returns and to \n\nCattaneo et al., 2020 for a theoretical study on the statistical properties of the sorting procedure (including theoretical links with regression-based approaches). Notably, the latter paper discusses the optimal number of portfolios and suggests that it is probably larger than the usual 10 often used in the literature.\n\nIn the code and Figure 3.1 below, we compute size portfolios (equally weighted: above versus below the median capitalization). According to the size anomaly, the firms with below median market cap should earn higher returns on average. This is verified whenever the orange bar in the plot is above the blue one (it happens most of the time).\n\ndf_median=[] # creating empty placeholder for temporary dataframe\ndf=[]\ndf_median=data_ml[['date','Size12m']].groupby(\n    ['date']).median().reset_index() # computing median\ndf_median.rename(\n    columns = {'Size12m': 'cap_median'}, inplace = True)\n# renaming for clarity\ndf = pd.merge(\n    data_ml[[\"date\",'Size12m','R1M']],\n    df_median,how='left', on=['date'])\ndf=df.groupby(\n    [pd.to_datetime(df['date']).dt.year,np.where(\n        df['Size12m'] > df['cap_median'],\n        'large', 'small')])['R1M'].mean().reset_index()\n# groupby and defining \"year\" and cap logic\ndf.rename(columns = {'level_1': 'cap_sort'}, inplace = True)\ndf.pivot(index='date',columns='cap_sort',\n         values='R1M').plot.bar(figsize=(10,6))\nplt.ylabel('Average returns')\nplt.xlabel('year')\ndf_median=[] #removing the temp dataframe to keep it light!\ndf=[]        #removing the temp dataframe to keep it light!\n\n\n\nFIGURE 3.1: The size factor: average returns of smaller versus larger firms.\n\n","type":"content","url":"/chap-03-asset-pricing#simple-portfolio-sorts","position":9},{"hierarchy":{"lvl1":"Factor investing and asset pricing anomalies","lvl3":"Factors","lvl2":"Detecting anomalies"},"type":"lvl3","url":"/chap-03-asset-pricing#factors","position":10},{"hierarchy":{"lvl1":"Factor investing and asset pricing anomalies","lvl3":"Factors","lvl2":"Detecting anomalies"},"content":"\n\nThe construction of so-called factors follows the same lines as above. Portfolios are based on one characteristic and the factor is a long-short ensemble of one extreme portfolio minus the opposite extreme (small minus large for the size factor or high book-to-market ratio minus low book-to-market ratio for the value factor). Sometimes, subtleties include forming bivariate sorts and aggregating several portfolios together, as in the original contribution of Fama and French (1993). The most common factors are listed below, along with a few references. We refer to the books listed at the beginning of the chapter for a more exhaustive treatment of factor idiosyncrasies. For most anomalies, theoretical justifications have been brought forward, whether risk-based or behavioral. We list the most frequently cited factors below:\n\nSize (SMB = small firms minus large firms): Banz (1981), Fama and French (1992), Fama and French (1993), Van Dijk (2011), Asness et al. (2018) and Astakhov, Havranek, and Novak (2019).\n\nValue (HML = high minus low: undervalued minus `growth’ firms): Fama and French (1992WML = winners minus losers): Jegadeesh and Titman (1993), Carhart (1997) and C), Fama and French (1993), C. S. Asness, Moskowitz, and Pedersen (2013).\n\nMomentum (WML = winners minus losers): Jegadeesh and Titman (1993), Carhart (1997) and C. S. Asness, Moskowitz, and Pedersen (2013). The winners are the assets that have experienced the highest returns over the last year (sometimes the computation of the return is truncated to omit the last month). Cross-sectional momentum is linked, but not equivalent, to time series momentum (trend following), see e.g., Moskowitz, Ooi, and Pedersen (2012) and Lempérière et al. (2014). Momentum is also related to contrarian movements that occur both at higher and lower frequencies (short-term and long-term reversals), see Luo, Subrahmanyam, and Titman (2020).\n\nProfitability (RMW = robust minus weak profits): Fama and French (2015), Bouchaud et al. (2019). In the former reference, profitability is measured as (revenues - (cost and expenses))/equity.\n\nInvestment (CMA = conservative minus aggressive): Fama and French (2015), Hou, Xue, and Zhang (2015). Investment is measured via the growth of total assets (divided by total assets). Aggressive firms are those that experience the largest growth in assets.\n\nLow `risk’ (sometimes, BAB = betting against beta): Ang et al. (2006), Baker, Bradley, and Wurgler (2011), Frazzini and Pedersen (2014), Boloorforoosh et al. (2020), Baker, Hoeyer, and Wurgler (2020) and Asness et al. (2020). In this case, the computation of risk changes from one article to the other (simple volatility, market beta, idiosyncratic volatility, etc.).\n\nWith the notable exception of the low risk premium, the most mainstream anomalies are kept and updated in the data library of Kenneth French (\n\nhttps://​mba​.tuck​.dartmouth​.edu​/pages​/faculty​/ken​.french​/data​_library​.html). Of course, the computation of the factors follows a particular set of rules, but they are generally accepted in the academic sphere. Another source of data is the AQR repository: \n\nhttps://​www​.aqr​.com​/Insights​/Datasets.\n\nIn the dataset we use for the book, we proxy the value anomaly not with the book-to-market ratio but with the price-to-book ratio (the book value is located in the denominator). As is shown in Clifford Asness and Frazzini (2013), the choice of the variable for value can have sizable effects.\n\nBelow, we import data from Ken French’s data library. We will use it later on in the chapter.\n\nmin_date = 196307\nmax_date = 202403\nff_url = \"https://mba.tuck.dartmouth.edu/pages/faculty/ken.french/ftp\"\nff_url += \"/F-F_Research_Data_5_Factors_2x3_CSV.zip\"\n# Create the download url\ndf_ff = pd.read_csv(ff_url, sep=',', skiprows=3, quotechar='\"')\ndf_ff.rename(columns = {'Unnamed: 0':'date'},\n             inplace = True) # renaming for clarity\ndf_ff.rename(columns = {'Mkt-RF':'MKT_RF'},\n             inplace = True) # renaming for clarity\ndf_ff = df_ff.iloc[0:743, :]             \ndf_ff[['date','MKT_RF','SMB','HML','RMW','CMA','RF']] = df_ff[['date','MKT_RF','SMB','HML','RMW','CMA','RF']].apply(pd.to_numeric)\ndf_ff[['MKT_RF','SMB','HML','RMW','CMA','RF']]=df_ff[\n    ['MKT_RF','SMB','HML','RMW','CMA','RF']].values/100.0 # Scale returns\nidx_ff=df_ff.index[(df_ff['date']>=min_date)&(\n    df_ff['date']<=max_date)].tolist()\nFF_factors=df_ff.iloc[idx_ff].copy()\nFF_factors.loc[:,'year']=FF_factors.date.astype(str).str[:4]\nFF_factors.iloc[1:6,0:7].head()\n\n\n\nTABLE 3.1: Sample of monthly factor returns.\n\nPosterior to the discovery of these stylized facts, some contributions have aimed at building theoretical models that capture these properties. We cite a handful below:\n\nsize and value: Berk, Green, and Naik (1999), K. D. Daniel, Hirshleifer, and Subrahmanyam (2001), Barberis and Shleifer (2003), Gomes, Kogan, and Zhang (2003), Carlson, Fisher, and Giammarino (2004), Arnott et al. (2014);\n\nmomentum: Johnson (2002), Grinblatt and Han (2005), Vayanos and Woolley (2013), Choi and Kim (2014).\n\nIn addition, recent bridges have been built between risk-based factor representations and behavioural theories. We refer essentially to Barberis, Mukherjee, and Wang (2016) and K. Daniel, Hirshleifer, and Sun (2020) and the references therein.\n\nWhile these factors (i.e., long-short portfolios) exhibit time-varying risk premia and are magnified by corporate news and announcements (Engelberg, McLean, and Pontiff (2018)), it is well-documented (and accepted) that they deliver positive returns over long horizons. We refer to Gagliardini, Ossola, and Scaillet (2016) and to the survey Gagliardini, Ossola, and Scaillet (2019), as well as to the related bibliography for technical details on estimation procedures of risk premia and the corresponding empirical results. A large sample study that documents regime changes in factor premia was also carried out by Ilmanen et al. (2019). Moreover, the predictability of returns is also time-varying (as documented in Farmer, Schmidt, and Timmermann (2019), Tsiakas, Li, and Zhang (2020) and Liu, Pan, and Wang (2020)), and estimation methods can be improved (Johnson (2019)).\n\nIn Figure 3.2, we plot the average monthly return aggregated over each calendar year for five common factors. The risk free rate (which is not a factor per se) is the most stable, while the market factor (aggregate market returns minus the risk-free rate) is the most volatile. This makes sense because it is the only long equity factor among the five series.\n\n# groupby and defining \"year\" and cap logic\nFF_factors.iloc[:,1:7].groupby(FF_factors['year']).mean().plot()\n# Group by year and factor, compute average return\nplt.ylabel('value') # Set axis name and plot!\nplt.xlabel('date')\n\n\n\nFIGURE 3.2: Average returns of common anomalies (1963-2020). Source: Ken French library.\n\nThe individual attributes of investors who allocate towards particular factors is a blossoming topic. We list a few references below, even though they somewhat lie out of the scope of this book. Betermier, Calvet, and Sodini (2017) show that value investors are older, wealthier and face lower income risk compared to growth investors who are those in the best position to take financial risks. The study Cronqvist, Siegel, and Yu (2015) leads to different conclusions: it finds that the propensity to invest in value versus growth assets has roots in genetics and in life events (the latter effect being confirmed in Cocco, Gomes, and Lopes (2020), and the former being further detailed in a more general context in Cronqvist et al. (2015)). Psychological traits can also explain some factors: when agents extrapolate, they are likely to fuel momentum (this topic is thoroughly reviewed in Barberis (2018)). Micro- and macro-economic consequences of these preferences are detailed in Bhamra and Uppal (2019). To conclude this paragraph, we mention that theoretical models have also been proposed that link agents’ preferences and beliefs (via prospect theory) to market anomalies (see for instance Barberis, Jin, and Wang (2020)).\n\nFinally, we highlight the need of replicability of factor premia and echo the recent editorial by Harvey (2020). As is shown by Linnainmaa and Roberts (2018) and Hou, Xue, and Zhang (2020), many proclaimed factors are in fact very much data-dependent and often fail to deliver sustained profitability when the investment universe is altered or when the definition of variable changes (Clifford Asness and Frazzini (2013)).\n\nCampbell Harvey and his co-authors, in a series of papers, tried to synthesize the research on factors in Harvey, Liu, and Zhu (2016), C. Harvey and Liu (2019) and Harvey and Liu (2019). His work underlines the need to set high bars for an anomaly to be called a ‘true’ factor. Increasing thresholds for p-values is only a partial answer, as it is always possible to resort to data snooping in order to find an optimized strategy that will fail out-of-sample but that will deliver a t-statistic larger than three (or even four). Harvey (2017) recommends to resort to a Bayesian approach which blends data-based significance with a prior into a so-called Bayesianized p-value (see subsection below).\n\nFollowing this work, researchers have continued to explore the richness of this zoo. Bryzgalova, Huang, and Julliard (2019) propose a tractable Bayesian estimation of large-dimensional factor models and evaluate all possible combinations of more than 50 factors, yielding an incredibly large number of coefficients. This combined with a Bayesianized Fama and MacBeth (1973) procedure allows to distinguish between pervasive and superfluous factors. Chordia, Goyal, and Saretto (2020) use simulations of 2 million trading strategies to estimate the rate of false discoveries, that is, when a spurious factor is detected (type I error). They also advise to use thresholds for t-statistics that are well above three. In a similar vein, Harvey and Liu (2020) also underline that sometimes true anomalies may be missed because of a one time t-statistic that is too low (type II error).\n\nThe propensity of journals to publish positive results has led researchers to estimate the difference between reported returns and true returns. A. Y. Chen and Zimmermann (2020) call this difference the publication bias and estimate it as roughly 12%. That is, if a published average return is 8%, the actual value may in fact be closer to (1-12%)*8%=7%. Qualitatively, this estimation of 12% is smaller than the out-of-sample reduction in returns found in McLean and Pontiff (2016).\n\n","type":"content","url":"/chap-03-asset-pricing#factors","position":11},{"hierarchy":{"lvl1":"Factor investing and asset pricing anomalies","lvl3":"Predictive regressions, sorts, and p-value issues","lvl2":"Detecting anomalies"},"type":"lvl3","url":"/chap-03-asset-pricing#predictive-regressions-sorts-and-p-value-issues","position":12},{"hierarchy":{"lvl1":"Factor investing and asset pricing anomalies","lvl3":"Predictive regressions, sorts, and p-value issues","lvl2":"Detecting anomalies"},"content":"\n\nFor simplicity, we assume a simple form:\n\n\\tag{3.2}\n\\textbf{r} = a+b\\textbf{x}+\\textbf{e},\n\nwhere the vector \\textbf{r} stacks all returns of all stocks and \\textbf{x} is a lagged variable so that the regression is indeed predictive. If the estimated \\hat{b} is significant given a specified threshold, then it can be tempting to conclude that \\textbf{x} does a good job at predicting returns. Hence, long-short portfolios related to extreme values of \\textbf{x} (mind the sign of \\hat{b}) are expected to generate profits. This is unfortunately often false because \\hat{b} gives information on the past* ability of \\textbf{x} to forecast returns. What happens in the future may be another story.\n\nStatistical tests are also used for portfolio sorts. Assume two extreme portfolios are expected to yield very different average returns (like very small cap versus very large cap, or strong winners versus bad losers). The portfolio returns are written r_t^+ and r_t^-. The simplest test for the mean is t=\\sqrt{T}\\frac{m_{r_+}-m_{r_-}}{\\sigma_{r_+-r_-}}, where T is the number of points and m_{r_\\pm} denotes the means of returns and \\sigma_{r_+-r_-} is the standard deviation of the difference between the two series, i.e., the volatility of the long-short portfolio. In short, the statistic can be viewed as a scaled Sharpe ratio (though usually these ratios are computed for long-only portfolios) and can in turn be used to compute p-values to assess the robustness of an anomaly. As is shown in Linnainmaa and Roberts (2018) and Hou, Xue, and Zhang (2020), many factors discovered by reasearchers fail to survive in out-of-sample tests.\n\nOne reason why people are overly optimistic about anomalies they detect is the widespread reverse interpretation of the p-value. Often, it is thought of as the probability of one hypothesis (e.g., my anomaly exists) given the data. In fact, it’s the opposite; it’s the likelihood of your data sample, knowing that the anomaly holds.\n\n\\begin{align*}\np-\\text{value} &= P[D|H] \\\\\n\\text{target prob.}& = P[H|D]=\\frac{P[D|H]}{P[D]}\\times P[H],\n\\end{align*}\n\nwhere H stands for hypothesis and D for data. The equality in the second row is a plain application of Bayes’ identity: the interesting probability is in fact a transform of the p-value.\n\nTwo articles (at least) discuss this idea. Harvey (2017) introduces Bayesianized p-values:\n\n\\tag{3.3}\n\\text{Bayesianized } p-\\text{value}=\\text{Bpv}= e^{-t^2/2}\\times\\frac{\\text{prior}}{1+e^{-t^2/2}\\times \\text{prior}} ,\n\nwhere t is the t-statistic obtained from the regression (i.e., the one that defines the p-value) and prior is the analyst’s estimation of the odds that the hypothesis (anomaly) is true. The prior is coded as follows. Suppose there is a p% chance that the null holds (i.e., (1-p)% for the anomaly). The odds are coded as p/(1-p). Thus, if the t-statistic is equal to 2 (corresponding to a p-value of 5% roughly) and the prior odds are equal to 6, then the Bpv is equal to e^{-2}\\times 6 \\times(1+e^{-2}\\times 6)^{-1}\\approx 0.448 and there is a 44.8% chance that the null is true. This interpretation stands in sharp contrast with the original p-value which cannot be viewed as a probability that the null holds. Of course, one drawback is that the level of the prior is crucial and solely user-specified.\n\nThe work of Chinco, Neuhierl, and Weber (2020) is very different but shares some key concepts, like the introduction of Bayesian priors in regression outputs. They show that coercing the predictive regression with an L^2 constraint (see the ridge regression in Chapter 5) amounts to introducing views on what the true distribution of b is. The stronger the constraint, the more the estimate \\hat{b} will be shrunk towards zero. One key idea in their work is the assumption of a distribution for the true b across many anomalies. It is assumed to be Gaussian and centered. The interesting parameter is the standard deviation: the larger it is, the more frequently significant anomalies are discovered. Notably, the authors show that this parameter changes through time and we refer to the original paper for more details on this subject.\n\n","type":"content","url":"/chap-03-asset-pricing#predictive-regressions-sorts-and-p-value-issues","position":13},{"hierarchy":{"lvl1":"Factor investing and asset pricing anomalies","lvl3":"Fama-Macbeth regressions","lvl2":"Detecting anomalies"},"type":"lvl3","url":"/chap-03-asset-pricing#fama-macbeth-regressions","position":14},{"hierarchy":{"lvl1":"Factor investing and asset pricing anomalies","lvl3":"Fama-Macbeth regressions","lvl2":"Detecting anomalies"},"content":"\n\nAnother detection method was proposed by Fama and MacBeth (1973) through a two-stage regression analysis of risk premia. The first stage is a simple estimation of the relationship (): the regressions are run on a stock-by-stock basis over the corresponding time series. The resulting estimates \\hat{\\beta}_{i,k} are then plugged into a second series of regressions:\n\n\\begin{align*}\nr_{t,n}= \\gamma_{t,0} + \\sum_{k=1}^K\\gamma_{t,k}\\hat{\\beta}_{n,k} + \\varepsilon_{t,n},\n\\end{align*}\n\nwhich are run date-by-date on the cross-section of assets. Theoretically, the betas would be known and the regression would be run on the \\beta_{n,k} instead of their estimated values. The \\hat{\\gamma}_{t,k} estimate the premia of factor k at time t. Under suitable distributional assumptions on the \\varepsilon_{t,n}, statistical tests can be performed to determine whether these premia are significant or not. Typically, the statistic on the time-aggregated (average) premia \\hat{\\gamma}_k=\\frac{1}{T}\\sum_{t=1}^T\\hat{\\gamma}_{t,k}:\n\n\\begin{align*}\nt_k=\\frac{\\hat{\\gamma}_k}{\\hat{\\sigma_k}/\\sqrt{T}}\n\\end{align*}\n\nis often used in pure Gaussian contexts to assess whether or not the factor is significant (\\hat{\\sigma}_k is the standard deviation of the \\hat{\\gamma}_{t,k}).\n\nWe refer to Jagannathan and Wang (1998) and Petersen (2009) for technical discussions on the biases and losses in accuracy that can be induced by standard ordinary least squares (OLS) estimations. Moreover, as the \\hat\\beta_{i,k}\nin the second-pass regression are estimates, a second level of errors can arise (the so-called errors in variables). The interested reader will find some extensions and solutions in Shanken (1992), Ang, Liu, and Schwarz (2018) and Jegadeesh et al. (2019).\n\nBelow, we perform Fama and MacBeth (1973) regressions on our sample. We start by the first pass: individual estimation of betas. We build a dedicated function below to automate the process.\n\nfrom pandas.tseries.offsets import MonthEnd\nstocks_list=list(returns.columns)\nFF_factors['date'] = pd.to_datetime(\n    FF_factors['date'], format='%Y%m', errors='coerce') + MonthEnd(0)\nFF_factors = FF_factors.dropna(subset=['date'])\nFF_factors['date'] = FF_factors['date'].dt.date.astype(str)\ndata_FM = pd.merge(returns.iloc[:,0].reset_index(),\n                   FF_factors.iloc[:,0:7],how='left',on=['date'])\ndata=FF_factors\ndata_FM.dropna(inplace=True)\n\nimport statsmodels.api as sm\nresults_params =[]\nreg_result=[]\ndf_res_full=[]\nfor i in range(len(returns.columns)):\n    Y=returns.iloc[:,i].shift(-1).reset_index()\n    Y=Y.drop(columns=['date'])\n    Y.dropna(inplace=True)\n    results=sm.OLS(endog=Y,exog=sm.add_constant(\n        data_FM.iloc[0:227,2:7])).fit()\n    results_params=results.params\n    reg_result_tmp=pd.DataFrame(results_params)\n    reg_result_tmp['stock_id']=stocks_list[i]\n    df_res_full.append(reg_result_tmp)\n\ndf_res_full = pd.concat(df_res_full)\ndf_res_full.reset_index(inplace=True)\ndf_res_full.rename(columns={\"index\":\"factors_name\",0:\"betas\"},inplace=True)\ndf_res_full_mat=df_res_full.pivot(index='stock_id',\n                                  columns='factors_name',values='betas')\ncolumn_names_inverted = [\"const\", \"MKT_RF\", \"SMB\",\"HML\",\"RMW\",\"CMA\"]\nreg_result = df_res_full_mat.reindex(columns=column_names_inverted)\n\n\n\nTABLE 3.2: Sample of beta values (row numbers are stock IDs).\n\nreg_result.head()\n\n\n\nIn the table, MKT_RF is the market return minus the risk free rate. The corresponding coefficient is often referred to as the beta, especially in univariate regressions. We then reformat these betas from Table 3.2 to prepare the second pass. Each line corresponds to one asset: the first 5 columns are the estimated factor loadings and the remaining ones are the asset returns (date by date).\n\nreturns_trsp=returns.transpose()\ndf_2nd_pass=pd.concat([reg_result.iloc[:,1:6],returns.transpose()],axis=1)\n\n\n\ndf_2nd_pass.head()\n\n\n\nTABLE 3.3: Sample of reformatted beta values (ready for regression).\n\nWe observe that the values of the first column (market betas) revolve around one, which is what we would expect. Finally, we are ready for the second round of regressions.\n\nbetas=df_2nd_pass.iloc[:,0:5]\ndate_list=list(returns_trsp.columns)\nresults_params=[]\nreg_result=[]\ndf_res_full=[]\nfor j in range(len(returns_trsp.columns)):\n    Y=returns_trsp.iloc[:,j]\n    results=sm.OLS(endog=Y,exog=sm.add_constant(betas)).fit()\n    results_params=results.params\n    reg_result_tmp=pd.DataFrame(results_params)\n    reg_result_tmp['date']=date_list[j]\n    df_res_full.append(reg_result_tmp)\n\ndf_res_full = pd.concat(df_res_full)\ndf_res_full.reset_index(inplace=True)\ngammas=df_res_full\n\ngammas.rename(columns={\"index\":\"factors_name\", 0: betas\"},inplace=True)\ngammas_mat=gammas.pivot(index='date',columns='factors_name',values='betas')\ncolumn_names_inverted = [\"const\", \"MKT_RF\", \"SMB\",\"HML\",\"RMW\",\"CMA\"]\ngammas_mat = gammas_mat.reindex(columns=column_names_inverted)\ngammas_mat.head()\n\n\n\nTABLE 3.4: Sample of gamma (premia) values.\n\nVisually, the estimated premia are also very volatile. We plot their estimated values for the market, SMB and HML factors.\n\ngammas_mat.iloc[:,1:4].plot(\n    figsize=(14,10), subplots=True,sharey=True, sharex=True)\n # Take gammas:\nplt.show() # Plot\n\n\n\nFIGURE 3.3: Time series plot of gammas (premia) in Fama-Macbeth regressions.\n\nThe two spikes at the end of the sample signal potential colinearity issues; two factors seem to compensate in an unclear aggregate effect. This underlines the usefulness of penalized estimates (see Chapter 5).\n\n","type":"content","url":"/chap-03-asset-pricing#fama-macbeth-regressions","position":15},{"hierarchy":{"lvl1":"Factor investing and asset pricing anomalies","lvl3":"Factor competition","lvl2":"Detecting anomalies"},"type":"lvl3","url":"/chap-03-asset-pricing#factor-competition","position":16},{"hierarchy":{"lvl1":"Factor investing and asset pricing anomalies","lvl3":"Factor competition","lvl2":"Detecting anomalies"},"content":"\n\nThe core purpose of factors is to explain the cross-section of stock returns. For theoretical and practical reasons, it is preferable if redundancies within factors are avoided. Indeed, redundancies imply collinearity which is known to perturb estimates (Belsley, Kuh, and Welsch (2005)). In addition, when asset managers decompose the performance of their returns into factors, overlaps (high absolute correlations) between factors yield exposures that are less interpretable; positive and negative exposures compensate each other spuriously.\n\nA simple protocol to sort out redundant factors is to run regressions of each factor against all others:\n\n\\tag{3.4}\nf_{t,k} = a_k +\\sum_{j\\neq k} \\delta_{k,j} f_{t,j} + \\epsilon_{t,k}.\n\nThe interesting metric is then the test statistic associated to the estimation of a_k. If a_k is significantly different from zero, then the cross-section of (other) factors fails to explain exhaustively the average return of factor k. Otherwise, the return of the factor can be captured by exposures to the other factors and is thus redundant.\n\nOne mainstream application of this technique was performed in Fama and French (2015), in which the authors show that the HML factor is redundant when taking into account four other factors (Market, SMB, RMW and CMA). Below, we reproduce their analysis on an updated sample. We start our analysis directly with the database maintained by Kenneth French.\n\nWe can run the regressions that determine the redundancy of factors via the procedure defined in Equation (3.4).\n\ndf_res_full=[]\nfor i in range(0,5):\n    factors_list_full = [\"MKT_RF\",\"SMB\",\"HML\",\"RMW\",\"CMA\"]\n    factors_list_tmp=factors_list_full\n    Y=FF_factors[factors_list_full[i]]\n    factors_list_tmp.remove(factors_list_full[i])\n    data=FF_factors[factors_list_tmp]\n    results=sm.OLS(endog=Y,exog=sm.add_constant(data)).fit()\n    results_param=results.params\n    reg_result_tmp=pd.DataFrame(results_param)\n    reg_result_tmp['factor_mnemo']=Y.name\n    reg_result_tmp['pvalue']=results.pvalues\n    df_res_full.append(reg_result_tmp)\n\ndf_res_full = pd.concat(df_res_full)\ndf_res_full.reset_index(inplace=True)\ndf_res_full.rename(columns={0: \"coeff\"},inplace=True)\n\n\n\nWe obtain the vector of α values from Equation (). Below, we format these figures along with p-value thresholds and export them in a summary table. The significance levels of coefficients is coded as follows:0<(***)<0.001<(**)<0.01<(*)<0.05\n\ndf_significance=df_res_full\nconditions = [(df_significance['pvalue'] > 0) & (\n    df_significance['pvalue'] < 0.001), # create a conditions' list\n(df_significance['pvalue']>0.001) & (df_significance['pvalue']<0.01),\n(df_significance['pvalue']>0.01) & (df_significance['pvalue']<0.05),\n(df_significance['pvalue'] > 0.05)]\n\nvaluest = ['(***)','(**)','(*)','na']\n# Values assign for each condition\n\n# create new column and use np.select to assign values\ndf_significance['significance']=np.select(conditions,valuest).astype(str)\ndf_significance['coeff']=round(df_significance.coeff,3)\ndf_significance['coeff_stars']=df_significance.coeff.astype(\n    str)+' '+df_significance.significance\n\n# display updated DataFrame in the right shape\nnew_index=['MKT_RF','SMB','HML','RMW','CMA']\ndf_significance_pivot=df_significance.pivot(\n    index='index',columns='factor_mnemo',values='coeff_stars').transpose()\ndf_significance_pivot= df_significance_pivot.reindex(\n    columns=column_names_inverted)\ndf_significance_pivot.reindex(new_index)\n\n\n\nTABLE 3.5: Factor competition among the Fama and French (2015) five factors.\n\nWe confirm that the HML factor remains redundant when the four others are present in the asset pricing model. The figures we obtain are very close to the ones in the original paper (Fama and French (2015)), which makes sense, since we only add 5 years to their initial sample.\n\nAt a more macro-level, researchers also try to figure out which models (i.e., combinations of factors) are the most likely, given the data empirically observed (and possibly given priors formulated by the econometrician). For instance, this stream of literature seeks to quantify to which extent the 3-factor model of Fama and French (1993) outperforms the 5 factors in Fama and French (2015). In this direction, De Moor, Dhaene, and Sercu (2015) introduce a novel computation for p-values that compare the relative likelihood that two models pass a zero-alpha test. More generally, the Bayesian method of Barillas and Shanken (2018) was subsequently improved by Chib, Zeng, and Zhao (2020).\n\nLastly, even the optimal number of factors is a subject of disagreement among conclusions of recent work. While the traditional literature focuses on a limited number (3-5) of factors, more recent research by DeMiguel et al. (2020), He, Huang, and Zhou (2020), Kozak, Nagel, and Santosh (2019) and Freyberger, Neuhierl, and Weber (2020) advocates the need to use at least 15 or more (in contrast, Kelly, Pruitt, and Su (2019) argue that a small number of latent factors may suffice). Green, Hand, and Zhang (2017) even find that the number of characteristics that help explain the cross-section of returns varies in time.\n\n","type":"content","url":"/chap-03-asset-pricing#factor-competition","position":17},{"hierarchy":{"lvl1":"Factor investing and asset pricing anomalies","lvl3":"Advanced techniques","lvl2":"Detecting anomalies"},"type":"lvl3","url":"/chap-03-asset-pricing#advanced-techniques","position":18},{"hierarchy":{"lvl1":"Factor investing and asset pricing anomalies","lvl3":"Advanced techniques","lvl2":"Detecting anomalies"},"content":"\n\nThe ever increasing number of factors combined to their importance in asset management has led researchers to craft more subtle methods in order to   organize’’ the so-called factor zoo and, more importantly, to detect spurious anomalies and compare different asset pricing model specifications. We list a few of them below. - Feng, Giglio, and Xiu (2020) combine LASSO selection with Fama-MacBeth regressions to test if new factor models are worth it. They quantify the gain of adding one new factor to a set of predefined factors and show that many factors reported in papers published in the 2010 decade do not add much incremental value;\n\nC. Harvey and Liu (2019) (in a similar vein) use bootstrap on orthogonalized factors. They make the case that correlations among predictors is a major issue and their method aims at solving this problem. Their lengthy procedure seeks to test if maximal additional contribution of a candidate variable is significant;\n\nFama and French (2018) compare asset pricing models through squared maximum Sharpe ratios;\n\nGiglio and Xiu (2019) estimate factor risk premia using a three-pass method based on principal component analysis;\n\nPukthuanthong, Roll, and Subrahmanyam (2018) disentangle priced and non-priced factors via a combination of principal component analysis and Fama and MacBeth (1973) regressions;\n\nGospodinov, Kan, and Robotti (2019) warn against factor misspecification (when spurious factors are included in the list of regressors). Traded factors (resp. macro-economic factors) seem more likely (resp. less likely) to yield robust identifications (see also Bryzgalova (2019)).\n\nThere is obviously no infallible method, but the number of contributions in the field highlights the need for robustness. This is evidently a major concern when crafting investment decisions based on factor intuitions. One major hurdle for short-term strategies is the likely time-varying feature of factors. We refer for instance to Ang and Kristensen (2012) and Cooper and Maio (2019) for practical results and to Gagliardini, Ossola, and Scaillet (2016) and S. Ma et al. (2020) for more theoretical treatments (with additional empirical results).\n\n","type":"content","url":"/chap-03-asset-pricing#advanced-techniques","position":19},{"hierarchy":{"lvl1":"Factor investing and asset pricing anomalies","lvl2":"Factors or characteristics?"},"type":"lvl2","url":"/chap-03-asset-pricing#factors-or-characteristics","position":20},{"hierarchy":{"lvl1":"Factor investing and asset pricing anomalies","lvl2":"Factors or characteristics?"},"content":"\n\nThe decomposition of returns into linear factor models is convenient because of its simple interpretation. There is nonetheless a debate in the academic literature about whether firm returns are indeed explained by exposure to macro-economic factors or simply by the characteristics of firms. In their early study, Lakonishok, Shleifer, and Vishny (1994) argue that one explanation of the value premium comes from incorrect extrapolation of past earning growth rates. Investors are overly optimistic about firms subject to recent profitability. Consequently, future returns are (also) driven by the core (accounting) features of the firm. The question is then to disentangle which effect is the most pronounced when explaining returns: characteristics versus exposures to macro-economic factors.\n\nIn their seminal contribution on this topic, Daniel and Titman (1997) provide evidence in favour of the former (two follow-up papers are K. Daniel, Titman, and Wei (2001) and Daniel and Titman (2012)). They show that firms with high book-to-market ratios or small capitalizations display higher average returns, even if they are negatively loaded on the HML or SMB factors. Therefore, it seems that it is indeed the intrinsic characteristics that matter, and not the factor exposure. For further material on characteristics’ role in return explanation or prediction, we refer to the following contributions: - Section 2.5.2. in Goyal (2012) surveys pre-2010 results on this topic;\n\nChordia, Goyal, and Shanken (2019) find that characteristics explain a larger proportion of variation in estimated expected returns than factor loadings;\n\nKozak, Nagel, and Santosh (2018) reconcile factor-based explanations of premia to a theoretical model in which some agents’ demands are sentiment driven;\n\nHan et al. (2019) show with penalized regressions that 20 to 30 characteristics (out of 94) are useful for the prediction of monthly returns of US stocks. Their methodology is interesting: they regress returns against characteristics to build forecasts and then regress the returns on the forecast to assess if they are reliable. The latter regression uses a LASSO-type penalization (see Chapter 5) so that useless characteristics are excluded from the model. The penalization is extended to elasticnet in Rapach and Zhou (2019).\n\nKelly, Pruitt, and Su (2019) and Kim, Korajczyk, and Neuhierl (2019) both estimate models in which factors are latent but loadings (betas) and possibly alphas depend on characteristics. Kirby (2020) generalizes the first approach by introducing regime-switching. In contrast, Lettau and Pelger (2020a) and Lettau and Pelger (2020b) estimate latent factors without any link to particular characteristics (and provide large sample asymptotic properties of their methods).\n\nIn the same vein as Hoechle, Schmid, and Zimmermann (2018), Gospodinov, Kan, and Robotti (2019) and Bryzgalova (2019) and discuss potential errors that arise when working with portfolio sorts that yield long-short returns. The authors show that in some cases, tests based on this procedure may be deceitful. This happens when the characteristic chosen to perform the sort is correlated with an external (unobservable) factor. They propose a novel regression-based approach aimed at bypassing this problem.\n\nMore recently and in a separate stream of literature, R. S. J. Koijen and Yogo (2019) have introduced a demand model in which investors form their portfolios according to their preferences towards particular firm characteristics. They show that this allows them to mimic the portfolios of large institutional investors. In their model, aggregate demands (and hence, prices) are directly linked to characteristics, not to factors. In a follow-up paper, R. S. Koijen, Richmond, and Yogo (2019) show that a few sets of characteristics suffice to predict future returns. They also show that, based on institutional holdings from the UK and the US, the largest investors are those who are the most influencial in the formation of prices. In a similar vein, Betermier, Calvet, and Jo (2019) derive an elegant (theoretical) general equilibrium model that generates some well-documented anomalies (size, book-to-market). The models of Arnott et al. (2014) and Alti and Titman (2019) are also able to theoretically generate known anomalies. Finally, in I. Martin and Nagel (2019), characteristics influence returns via the role they play in the predictability of dividend growth. This paper discussed the asymptotic case when the number of assets and the number of characteristics are proportional and both increase to infinity.\n\n","type":"content","url":"/chap-03-asset-pricing#factors-or-characteristics","position":21},{"hierarchy":{"lvl1":"Factor investing and asset pricing anomalies","lvl2":"Hot Topics: momentum, timing and ESG"},"type":"lvl2","url":"/chap-03-asset-pricing#hot-topics-momentum-timing-and-esg","position":22},{"hierarchy":{"lvl1":"Factor investing and asset pricing anomalies","lvl2":"Hot Topics: momentum, timing and ESG"},"content":"\n\n","type":"content","url":"/chap-03-asset-pricing#hot-topics-momentum-timing-and-esg","position":23},{"hierarchy":{"lvl1":"Factor investing and asset pricing anomalies","lvl3":"Factor momentum","lvl2":"Hot Topics: momentum, timing and ESG"},"type":"lvl3","url":"/chap-03-asset-pricing#factor-momentum","position":24},{"hierarchy":{"lvl1":"Factor investing and asset pricing anomalies","lvl3":"Factor momentum","lvl2":"Hot Topics: momentum, timing and ESG"},"content":"\n\nA recent body of literature unveils a time series momentum property of factor returns. For instance, Gupta and Kelly (2019) report that autocorrelation patterns within these returns is statistically significant.7 Similar results are obtained in Falck, Rej, and Thesmar (2020). In the same vein, Arnott et al. (2020) make the case that the industry momentum found in Moskowitz and Grinblatt (1999) can in fact be explained by this factor momentum. Going even further, Ehsani and Linnainmaa (2019) conclude that the original momentum factor is in fact the aggregation of the autocorrelation that can be found in all other factors. Given the data obtained on Ken French’s website, we compute the autocorrelation function (ACF) of factors. We recall that\n\nAcknowledging the profitability of factor momentum, H. Yang (2020b) seeks to understand its source and decomposes stock factor momentum portfolios into two components: factor timing portfolio and a static portfolio. The former seeks to profit from the serial correlations of factor returns while the latter tries to harness factor premia. The author shows that it is the static portfolio that explains the larger portion of factor momentum returns. In H. Yang (2020a), the same author presents a new estimator to gauge factor momentum predictability. Words of caution are provided in Leippold and Yang (2021).\n\nLastly, Garcia, Medeiros, and Ribeiro (2021) document factor momentum at the daily frequency.\n\nGiven the data obtained on Ken French’s website, we compute the autocorrelation function (ACF) of factors. We recall that\n\n\\begin{align*}\n\\text{ACF}_k(\\textbf{x}_t)=\\mathbb{E}[(\\textbf{x}_t-\\bar{\\textbf{x}})(\\textbf{x}_{t+k}-\\bar{\\textbf{x}})].\n\\end{align*}\n\nfig, ax = plt.subplots(2,2,figsize=(10,5),sharex='all', sharey='all')\n# subplot and sharing axes\nsm.graphics.tsa.plot_acf(FF_factors.RMW, lags=10, ax=ax[0,0],title='RMW')\nsm.graphics.tsa.plot_acf(FF_factors.CMA, lags=10, ax=ax[1,0],title='CMA')\nsm.graphics.tsa.plot_acf(FF_factors.SMB, lags=10, ax=ax[0,1],title='SMB')\nsm.graphics.tsa.plot_acf(FF_factors.HML, lags=10, ax=ax[1,1],title='HML')\nplt.show()\n\n\n\nFIGURE 3.4: Autocorrelograms of common factor portfolios.\n\nOf the four chosen series, only the size factor is not significantly autocorrelated at the first order.\n\n","type":"content","url":"/chap-03-asset-pricing#factor-momentum","position":25},{"hierarchy":{"lvl1":"Factor investing and asset pricing anomalies","lvl3":"Factor timing","lvl2":"Hot Topics: momentum, timing and ESG"},"type":"lvl3","url":"/chap-03-asset-pricing#factor-timing","position":26},{"hierarchy":{"lvl1":"Factor investing and asset pricing anomalies","lvl3":"Factor timing","lvl2":"Hot Topics: momentum, timing and ESG"},"content":"\n\nGiven the abundance of evidence of the time-varying nature of factor premia, it is legitimate to wonder if it is possible to predict when factor will perform well or badly. The evidence on the effectiveness of timing is diverse: positive for Greenwood and Hanson (2012), Hodges et al. (2017), Hasler, Khapko, and Marfe (2019), Haddad, Kozak, and Santosh (2020) and Lioui and Tarelli (2020), negative for Asness et al. (2017) and mixed for Dichtl et al. (2019). There is no consensus on which predictors to use (general macroeconomic indicators in Hodges et al. (2017), stock issuances versus repurchases in Greenwood and Hanson (2012), and aggregate fundamental data in Dichtl et al. (2019)). A method for building reasonable timing strategies for long-only portfolios with sustainable transaction costs is laid out in Leippold and Rüegg (2020). In ML-based factor investing, it is possible to resort to more granularity by combining firm-specific attributes to large-scale economic data as we explain in Section 4.7.2.\n\n","type":"content","url":"/chap-03-asset-pricing#factor-timing","position":27},{"hierarchy":{"lvl1":"Factor investing and asset pricing anomalies","lvl3":"The green factors","lvl2":"Hot Topics: momentum, timing and ESG"},"type":"lvl3","url":"/chap-03-asset-pricing#the-green-factors","position":28},{"hierarchy":{"lvl1":"Factor investing and asset pricing anomalies","lvl3":"The green factors","lvl2":"Hot Topics: momentum, timing and ESG"},"content":"\n\nThe demand for ethical financial products has sharply risen during the 2010 decade, leading to the creation of funds dedicated to socially responsible investing (SRI - see Camilleri (2020)). Though this phenomenon is not really new (Schueth (2003), Hill et al. (2007)), its acceleration has prompted research about whether or not characteristics related to ESG criteria (environment, social, governance) are priced. Dozens and even possibly hundreds of papers have been devoted to this question, but no consensus has been reached. More and more, researchers study the financial impact of climate change (see Bernstein, Gustafson, and Lewis (2019), Hong, Li, and Xu (2019) and Hong, Karolyi, and Scheinkman (2020)) and the societal push for responsible corporate behavior (Fabozzi (2020), Kurtz (2020)). We gather below a very short list of papers that suggests conflicting results:\n\nfavorable: ESG investing works (Kempf and Osthoff (2007), Cheema-Fox et al. (2020)), can work (Nagy, Kassam, and Lee (2016), Alessandrini and Jondeau (2020)), or can at least be rendered efficient (Branch and Cai (2012)). A large meta-study reports overwhelming favorable results (Friede, Busch, and Bassen (2015)), but of course, they could well stem from the publication bias towards positive results.\n\nunfavorable: Ethical investing is not profitable according to Adler and Kritzman (2008) and Blitz and Swinkels (2020). An ESG factor should be long unethical firms and short ethical ones (Lioui (2018)).\n\nmixed: ESG investing may be beneficial globally but not locally (Chakrabarti and Sen (2020)). Portfolios relying on ESG screening do not significantly outperform those with no screening but are subject to lower levels of volatility (Gibson et al. (2020), Gougler and Utz (2020)). As is often the case, the devil is in the details, and results depend on whether to use E, S or G (Bruder et al. (2019)).\n\nOn top of these contradicting results, several articles point towards complexities in the measurement of ESG. Depending on the chosen criteria and on the data provider, results can change drastically (see Galema, Plantinga, and Scholtens (2008), Berg, Koelbel, and Rigobon (2020) and Atta-Darkua et al. (2020)).\n\nWe end this short section by noting that of course ESG criteria can directly be integrated into ML model, as is for instance done in Franco et al. (2020).\n\n","type":"content","url":"/chap-03-asset-pricing#the-green-factors","position":29},{"hierarchy":{"lvl1":"Factor investing and asset pricing anomalies","lvl2":"The links with machine learning"},"type":"lvl2","url":"/chap-03-asset-pricing#the-links-with-machine-learning","position":30},{"hierarchy":{"lvl1":"Factor investing and asset pricing anomalies","lvl2":"The links with machine learning"},"content":"\n\nGiven the exponential increase in data availability, the obvious temptation of any asset manager is to try to infer future returns from the abundance of attributes available at the firm level. We allude to classical data like accounting ratios and to alternative data, such as sentiment. This task is precisely the aim of Machine Learning. Given a large set of predictor variables (\\mathbf{X}), the goal is to predict a proxy for future performance \\mathbf{y} through a model of the form (2.1).\n\nSome attempts toward this direction have already been made (e.g., Brandt, Santa-Clara, and Valkanov (2009), Hjalmarsson and Manchev (2012), Ammann, Coqueret, and Schade (2016), DeMiguel et al. (2020)), but not with any ML intent or focus originally. In retrospect, these approaches do share some links with ML tools. The general formulation is the following. At timeT, the agent or investor seeks to solve the following program:\n\n\\begin{align*}\n\\underset{\\boldsymbol{\\theta}_T}{\\max} \\ \\mathbb{E}_T\\left[ u(r_{p,T+1})\\right] = \\underset{\\boldsymbol{\\theta}_T}{\\max} \\ \\mathbb{E}_T\\left[ u\\left(\\left(\\bar{\\textbf{w}}_T+\\textbf{x}_T\\boldsymbol{\\theta}_T\\right)'\\textbf{r}_{T+1}\\right)\\right] ,\n\\end{align*}\n\nwhere u is some utility function and r_{p,T+1}=\\left(\\bar{\\textbf{w}}_T+\\textbf{x}_T\\boldsymbol{\\theta}_T\\right)'\\textbf{r}_{T+1} is the return of the portfolio, which is defined as a benchmark \\bar{\\textbf{w}}_T plus some deviations from this benchmark that are a linear function of features \\textbf{x}_T\\boldsymbol{\\theta}_T. The above program may be subject to some external constraints (e.g., to limit leverage).\n\nIn practice, the vector \\boldsymbol{\\theta}_T must be estimated using past data (from T-\\tau to T-1): the agent seeks the solution of\n\n\\begin{align}\n\\tag{3.5}\n\\underset{\\boldsymbol{\\theta}_T}{\\text{max}} \\ \\frac{1}{\\tau} \\sum_{t=T-\\tau}^{T-1} u \\left( \\sum_{i=1}^{N_T}\\left(\\bar{w}_{i,t}+ \\boldsymbol{\\theta}'_T \\textbf{x}_{i,t} \\right)r_{i,t+1} \\right)\n\\end{align}\n\non a sample of size τ where N_T is the number of asset in the universe. The above formulation can be viewed as a learning task in which the parameters are chosen such that the reward (average return) is maximized.\n\n","type":"content","url":"/chap-03-asset-pricing#the-links-with-machine-learning","position":31},{"hierarchy":{"lvl1":"Factor investing and asset pricing anomalies","lvl3":"A short list of recent references","lvl2":"The links with machine learning"},"type":"lvl3","url":"/chap-03-asset-pricing#a-short-list-of-recent-references","position":32},{"hierarchy":{"lvl1":"Factor investing and asset pricing anomalies","lvl3":"A short list of recent references","lvl2":"The links with machine learning"},"content":"\n\nIndependent of a characteristics-based approach, ML applications in finance have blossomed, initially working with price data only and later on integrating firm characteristics as predictors. We cite a few references below, grouped by methodological approach:\n\npenalized quadratic programming: Goto and Xu (2015), Ban, El Karoui, and Lim (2016) and Perrin and Roncalli (2019),\n\nregularized predictive regressions: Rapach, Strauss, and Zhou (2013) and Alexander Chinco, Clark-Joseph, and Ye (2019),\n\nsupport vector machines: Cao and Tay (2003) (and the references therein),\n\nmodel comparison and/or aggregation: Kim (2003), Huang, Nakamori, and Wang (2005), Matı́as and Reboredo (2012), Reboredo, Matı́as, and Garcia-Rubio (2012), Dunis et al. (2013), Gu, Kelly, and Xiu (2020b) and Guida and Coqueret (2018b). The latter two more recent articles work with a large cross-section of characteristics.\n\nWe provide more detailed lists for tree-based methods, neural networks and reinforcement learning techniques in Chapters 6, 7 and 16, respectively. Moreover, we refer to Ballings et al. (2015) for a comparison of classifiers and to Henrique, Sobreiro, and Kimura (2019) and Bustos and Pomares-Quimbaya (2020) for surveys on ML-based forecasting techniques.\n\n","type":"content","url":"/chap-03-asset-pricing#a-short-list-of-recent-references","position":33},{"hierarchy":{"lvl1":"Factor investing and asset pricing anomalies","lvl3":"Explicit connections with asset pricing models","lvl2":"The links with machine learning"},"type":"lvl3","url":"/chap-03-asset-pricing#explicit-connections-with-asset-pricing-models","position":34},{"hierarchy":{"lvl1":"Factor investing and asset pricing anomalies","lvl3":"Explicit connections with asset pricing models","lvl2":"The links with machine learning"},"content":"\n\nThe first and obvious link between factor investing and asset pricing is (average) return prediction. The main canonical academic reference is Gu, Kelly, and Xiu (2020b). Let us first write the general equation and then comment on it:\n\n\\tag{3.6}\nr_{t+1,n}=g(\\textbf{x}_{t,n}) + \\epsilon_{t+1}.\n\nThe interesting discussion lies in the differences between the above model and that of Equation (3.1). The first obvious difference is the introduction of the nonlinear function g: indeed, there is no reason (beyond simplicity and interpretability) why we should restrict the model to linear relationships. One early reference for nonlinearities in asset pricing kernels is Bansal and Viswanathan (1993).\n\nMore importantly, the second difference between (3.6) and (3.1) is the shift in the time index. Indeed, from an investor’s perspective, the interest is to be able to predict some information about the structure of the cross-section of assets. Explaining asset returns with synchronous factors is not useful because the realization of factor values is not known in advance. Hence, if one seeks to extract value from the model, there needs to be a time interval between the observation of the state space (which we call \\textbf{x}_{t,n}) and the occurrence of the returns. Once the model \\hat{g} is estimated, the time-t (measurable) value g(\\textbf{x}_{t,n}) will give a forecast for the (average) future returns. These predictions can then serve as signals in the crafting of portfolio weights (see Chapter 12 for more on that topic).\n\nWhile most studies do work with returns on the l.h.s. of (3.6), there is no reason why other indicators should not be used. Returns are straightforward and simple to compute, but they could very well be replaced by more sophisticated metrics, like the Sharpe ratio, for instance. The firms’ features would then be used to predict a risk-adjusted performance rather than simple returns.\n\nBeyond the explicit form of Equation (3.6), several other ML-related tools can also be used to estimate asset pricing models. This can be achieved in several ways, some of which we list below.\n\nFirst, one mainstream problem in asset pricing is to characterize the stochastic discount factor (SDF) M_t, which satisfies \\mathbb{E}_t[M_{t+1}(r_{t+1,n}-r_{t+1,f})]=0 for any asset n (see Cochrane (2009)). This equation is a natural playing field for the generalized method of moment (Hansen (1982)): M_t must be such that\n\n\\tag{3.7}\n\\mathbb{E}[M_{t+1}R_{t+1,n}g(V_t)]=0,\n\nwhere the instrumental variables V_t are \\mathcal{F}_t-measurable (i.e., are known at time t) and the capital R_{t+1,n} denotes the excess return of asset n. In order to reduce and simplify the estimation problem, it is customary to define the SDF as a portfolio of assets (see chapter 3 in Back (2010)). In Luyang Chen, Pelger, and Zhu (2020), the authors use a generative adversarial network (GAN, see Section 7.7.1) to estimate the weights of the portfolios that are the closest to satisfy (3.7) under a strongly penalizing form.\n\nA second approach is to try to model asset returns as linear combinations of factors, just as in (3.1). We write in compact notation\n\n\\begin{align*}\nr_{t,n}=\\alpha_n+\\boldsymbol{\\beta}_{t,n}'\\textbf{f}_t+\\epsilon_{t,n},\n\\end{align*}\n\nand we allow the loadings \\boldsymbol{\\beta}_{t,n} to be time-dependent. The trick is then to introduce the firm characteristics in the above equation. Traditionally, the characteristics are present in the definition of factors (as in the seminal definition of Fama and French (1993)). The decomposition of the return is made according to the exposition of the firm’s return to these factors constructed according to market size, accounting ratios, past performance, etc. Given the exposures, the performance of the stock is attributed to particular style profiles (e.g., small stock, or value stock, etc.).\n\nHabitually, the factors are heuristic portfolios constructed from simple rules like thresholding. For instance, firms below the 1/3 quantile in book-to-market are growth firms and those above the 2/3 quantile are the value firms. A value factor can then be defined by the long-short portfolio of these two sets, with uniform weights. Note that Fama and French (1993) use a more complex approach which also takes market capitalization into account both in the weighting scheme and also in the composition of the portfolios.\n\nOne of the advances enabled by machine learning is to automate the construction of the factors. It is for instance the approach of Feng, Polson, and Xu (2019). Instead of building the factors heuristically, the authors optimize the construction to maximize the fit in the cross-section of returns. The optimization is performed via a relatively deep feed-forward neural network and the feature space is lagged so that the relationship is indeed predictive, as in Equation (3.6). Theoretically, the resulting factors help explain a substantially larger proportion of the in-sample variance in the returns. The prediction ability of the model depends on how well it generalizes out-of-sample.\n\nA third approach is that of Kelly, Pruitt, and Su (2019) (though the statistical treatment is not machine learning per se). Their idea is the opposite: factors are latent (unobserved) and it is the betas (loadings) that depend on the characteristics. This allows many degrees of freedom because in r_{t,n}=\\alpha_n+(\\boldsymbol{\\beta}_{t,n}(\\textbf{x}_{t-1,n}))'\\textbf{f}_t+\\epsilon_{t,n},, only the characteristics \\textbf{x}_{t-1,n} are known and both the factors \\textbf{f}_t and the functional forms \\boldsymbol{\\beta}_{t,n}(\\cdot) must be estimated. In their article, Kelly, Pruitt, and Su (2019) work with a linear form, which is naturally more tractable.\n\nLastly, a fourth approach (introduced in Gu, Kelly, and Xiu (2020a)) goes even further and combines two neural network architectures. The first neural network takes characteristics \\textbf{x}_{t-1} as inputs and generates factor loadings \\boldsymbol{\\beta}_{t-1}(\\textbf{x}_{t-1}). The second network transforms returns \\textbf{r}_t into factor values \\textbf{f}_t(\\textbf{r}_t) (in Feng, Polson, and Xu (2019)). The aggregate model can then be written:\n\n\\tag{3.8}\n\\textbf{r}_t=\\boldsymbol{\\beta}_{t-1}(\\textbf{x}_{t-1})'\\textbf{f}_t(\\textbf{r}_t)+\\boldsymbol{\\epsilon}_t.\n\nThe above specification is quite special because the output (on the l.h.s.) is also present as input (in the r.h.s.). In machine learning, autoencoders (see Section 7.6.2) share the same property. Their aim, just like in principal component analysis, is to find a parsimonious nonlinear representation form for a dataset (in this case, returns). In Equation (3.8), the input is \\textbf{r}_t and the output function is \\boldsymbol{\\beta}_{t-1}(\\textbf{x}_{t-1})'\\textbf{f}_t(\\textbf{r}_t). The aim is to minimize the difference between the two just as is any regression-like model.\n\nAutoencoders are neural networks which have outputs as close as possible to the inputs with an objective of dimensional reduction. The innovation in Gu, Kelly, and Xiu (2020a) is that the pure autoencoder part is merged with a vanilla perceptron used to model the loadings. The structure of the neural network is summarized below.\n\n\\left. \\begin{array}{rl}\n\\text{returns } (\\textbf{r}_t) & \\overset{NN_1}{\\longrightarrow} \\quad \\text{ factors } (\\textbf{f}_t=NN_1(\\textbf{r}_t)) \\\\\n\\text{characteristics } (\\textbf{x}_{t-1}) & \\overset{NN_2}{\\longrightarrow} \\quad \\text{ loadings } (\\boldsymbol{\\beta}_{t-1}=NN_2(\\textbf{x}_{t-1}))\n\\end{array} \\right\\} \\longrightarrow \\text{ returns } (r_t)\n\nA simple autoencoder would consist of only the first line of the model. This specification is discussed in more details in Section 7.6.2.\n\nAs a conclusion of this chapter, it appears undeniable that the intersection between the two fields of asset pricing and machine learning offers a rich variety of applications. The literature is already exhaustive and it is often hard to disentangle the noise from the great ideas in the continuous flow of publications on these topics. Practice and implementation is the only way forward to extricate value from hype. This is especially true because agents often tend to overestimate the role of factors in the allocation decision process of real-world investors (see Alex Chinco, Hartzmark, and Sussman (2019) and Castaneda and Sabat (2019)).\n\n","type":"content","url":"/chap-03-asset-pricing#explicit-connections-with-asset-pricing-models","position":35},{"hierarchy":{"lvl1":"Factor investing and asset pricing anomalies","lvl2":"Coding Exercices"},"type":"lvl2","url":"/chap-03-asset-pricing#coding-exercices","position":36},{"hierarchy":{"lvl1":"Factor investing and asset pricing anomalies","lvl2":"Coding Exercices"},"content":"\n\nCompute annual returns of the growth versus value portfolios, that is, the average return of firms with above median price-to-book ratio (the variable is called `Pb’ in the dataset).\n\nSame exercise, but compute the monthly returns and plot the value (through time) of the corresponding portfolios.\n\nInstead of a unique threshold, compute simply sorted portfolios based on quartiles of market capitalization. Compute their annual returns and plot them.","type":"content","url":"/chap-03-asset-pricing#coding-exercices","position":37}]}